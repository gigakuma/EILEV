{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Ego4dFHOClipDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "# helpful functions\n",
    "\n",
    "\n",
    "def draw_random_items(dataset, num):\n",
    "    return [dataset[idx] for idx in random.sample(list(range(len(dataset))), num)]\n",
    "\n",
    "\n",
    "def display_gif(video_tensor, gif_file_name):\n",
    "    \"\"\"Prepares and displays a GIF from a video tensor.\n",
    "\n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_channels, num_frames, height, width).\n",
    "    \"\"\"\n",
    "    iio.imwrite(\n",
    "        gif_file_name,\n",
    "        video_tensor.permute(1, 2, 3, 0).numpy().astype(np.uint8),\n",
    "        extension=\".gif\",\n",
    "        # infinite loop\n",
    "        loop=0,\n",
    "    )\n",
    "    return Image(gif_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and perform common preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "import re\n",
    "\n",
    "from train import clean_narration_text, load_ego4d_fho_clip_dataset\n",
    "\n",
    "C_REGEX = re.compile(r\"^\\#C C\", re.IGNORECASE)\n",
    "\n",
    "annotation_path = \"../../ego4d/v2/annotations/fho_main.json\"\n",
    "dataset = load_ego4d_fho_clip_dataset(annotation_path)\n",
    "\n",
    "# filter out rejected, invalid and non-C actions\n",
    "dataset = dataset.filter(\n",
    "    lambda is_rejected, is_valid_action, narration_text: not is_rejected\n",
    "    and is_valid_action\n",
    "    and C_REGEX.match(narration_text),\n",
    "    input_columns=[\"is_rejected\", \"is_valid_action\", \"narration_text\"],\n",
    ")\n",
    "\n",
    "# remove unused columns\n",
    "dataset = dataset.remove_columns(\n",
    "    [\n",
    "        \"warnings\",\n",
    "        \"uid\",\n",
    "        \"start_sec\",\n",
    "        \"end_sec\",\n",
    "        \"start_frame\",\n",
    "        \"end_frame\",\n",
    "        \"is_valid_action\",\n",
    "        \"is_partial\",\n",
    "        \"clip_start_frame\",\n",
    "        \"clip_end_frame\",\n",
    "        \"narration_timestamp_sec\",\n",
    "        \"clip_narration_timestamp_sec\",\n",
    "        \"narration_annotation_uid\",\n",
    "        \"structured_verb\",\n",
    "        \"freeform_verb\",\n",
    "        \"state_transition\",\n",
    "        \"critical_frames\",\n",
    "        \"clip_critical_frames\",\n",
    "        \"frames\",\n",
    "        \"is_rejected\",\n",
    "        \"is_invalid_annotation\",\n",
    "        \"reject_reason\",\n",
    "        \"stage\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    clean_narration_text,\n",
    "    input_columns=\"narration_text\",\n",
    "    remove_columns=\"narration_text\",\n",
    ")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform LM-specific preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from train import add_prompt_column, generate_inputs\n",
    "\n",
    "# instruction tuned and decoder only LMs\n",
    "instr_tuned_decoder_only_lm_dataset = dataset.map(\n",
    "    partial(add_prompt_column, instruct_tuned=True)\n",
    ").map(\n",
    "    partial(generate_inputs, use_decoder_only_lm=True),\n",
    "    remove_columns=[\"prompt\", \"cleaned_narration_text\"],\n",
    ")\n",
    "print(\"instr_tuned_decoder_only_lm_dataset\")\n",
    "for item in draw_random_items(instr_tuned_decoder_only_lm_dataset, 3):\n",
    "    print(item)\n",
    "print(\"===================================\")\n",
    "\n",
    "# instruction tuned and seq2seq LMs\n",
    "instr_tuned_seq2seq_lm_dataset = dataset.map(\n",
    "    partial(add_prompt_column, instruct_tuned=True)\n",
    ").map(\n",
    "    partial(generate_inputs, use_decoder_only_lm=False),\n",
    "    remove_columns=[\"prompt\", \"cleaned_narration_text\"],\n",
    ")\n",
    "print(\"instr_tuned_seq2seq_lm_dataset \")\n",
    "for item in draw_random_items(instr_tuned_seq2seq_lm_dataset, 3):\n",
    "    print(item)\n",
    "print(\"===================================\")\n",
    "\n",
    "# non-instruction tuned and decoder only LMs\n",
    "non_instr_tuned_decoder_only_lm_dataset = dataset.map(\n",
    "    partial(add_prompt_column, instruct_tuned=False)\n",
    ").map(\n",
    "    partial(generate_inputs, use_decoder_only_lm=True),\n",
    "    remove_columns=[\"prompt\", \"cleaned_narration_text\"],\n",
    ")\n",
    "print(\"non_instr_tuned_decoder_only_lm_dataset\")\n",
    "for item in draw_random_items(non_instr_tuned_decoder_only_lm_dataset, 3):\n",
    "    print(item)\n",
    "print(\"===================================\")\n",
    "\n",
    "# non-instruction tuned and seq2seq LMs\n",
    "non_instr_tuned_seq2seq_lm_dataset = dataset.map(\n",
    "    partial(add_prompt_column, instruct_tuned=False)\n",
    ").map(\n",
    "    partial(generate_inputs, use_decoder_only_lm=False),\n",
    "    remove_columns=[\"prompt\", \"cleaned_narration_text\"],\n",
    ")\n",
    "print(\"non_instr_tuned_seq2seq_lm_dataset\")\n",
    "for item in draw_random_items(non_instr_tuned_seq2seq_lm_dataset, 3):\n",
    "    print(item)\n",
    "print(\"===================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from transformers import Blip2Processor\n",
    "\n",
    "from train import batch_tokenize\n",
    "\n",
    "decoder_only_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "seq2seq_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "\n",
    "# instruction tuned and decoder only LMs\n",
    "instr_tuned_decoder_only_lm_dataset = instr_tuned_decoder_only_lm_dataset.map(\n",
    "    partial(batch_tokenize, decoder_only_processor.tokenizer, use_decoder_only_lm=True),\n",
    "    batched=True,\n",
    "    remove_columns=\"input\",\n",
    ")\n",
    "print(\"instr_tuned_decoder_only_lm_dataset\")\n",
    "for item in draw_random_items(instr_tuned_decoder_only_lm_dataset, 3):\n",
    "    print(item)\n",
    "print(\"===================================\")\n",
    "\n",
    "# instruction tuned and seq2seq LMs\n",
    "instr_tuned_seq2seq_lm_dataset = instr_tuned_seq2seq_lm_dataset.map(\n",
    "    partial(batch_tokenize, seq2seq_processor.tokenizer, use_decoder_only_lm=False),\n",
    "    batched=True,\n",
    "    remove_columns=\"input\",\n",
    ")\n",
    "print(\"instr_tuned_seq2seq_lm_dataset \")\n",
    "for item in draw_random_items(instr_tuned_seq2seq_lm_dataset, 3):\n",
    "    print(item)\n",
    "print(\"===================================\")\n",
    "\n",
    "# non-instruction tuned and decoder only LMs\n",
    "non_instr_tuned_decoder_only_lm_dataset = non_instr_tuned_decoder_only_lm_dataset.map(\n",
    "    partial(batch_tokenize, decoder_only_processor.tokenizer, use_decoder_only_lm=True),\n",
    "    batched=True,\n",
    "    remove_columns=\"input\",\n",
    ")\n",
    "print(\"non_instr_tuned_decoder_only_lm_dataset\")\n",
    "for item in draw_random_items(non_instr_tuned_decoder_only_lm_dataset, 3):\n",
    "    print(item)\n",
    "print(\"===================================\")\n",
    "\n",
    "# non-instruction tuned and seq2seq LMs\n",
    "non_instr_tuned_seq2seq_lm_dataset = non_instr_tuned_seq2seq_lm_dataset.map(\n",
    "    partial(batch_tokenize, seq2seq_processor.tokenizer, use_decoder_only_lm=False),\n",
    "    batched=True,\n",
    "    remove_columns=\"input\",\n",
    ")\n",
    "print(\"non_instr_tuned_seq2seq_lm_dataset\")\n",
    "for item in draw_random_items(non_instr_tuned_seq2seq_lm_dataset, 3):\n",
    "    print(item)\n",
    "print(\"===================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and val sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruction tuned and decoder only LMs\n",
    "instr_tuned_decoder_only_lm_dataset_train_val = (\n",
    "    instr_tuned_decoder_only_lm_dataset.train_test_split(test_size=0.1, shuffle=True)\n",
    ")\n",
    "print(\n",
    "    \"instr_tuned_decoder_only_lm_dataset_train_val: \"\n",
    "    f\"{instr_tuned_decoder_only_lm_dataset_train_val}\"\n",
    ")\n",
    "\n",
    "# instruction tuned and seq2seq LMs\n",
    "instr_tuned_seq2seq_lm_dataset_train_val = (\n",
    "    instr_tuned_seq2seq_lm_dataset.train_test_split(test_size=0.1, shuffle=True)\n",
    ")\n",
    "print(\n",
    "    \"instr_tuned_seq2seq_lm_dataset_train_val: \"\n",
    "    f\"{instr_tuned_seq2seq_lm_dataset_train_val }\"\n",
    ")\n",
    "\n",
    "# non-instruction tuned and decoder only LMs\n",
    "non_instr_tuned_decoder_only_lm_dataset_train_val = (\n",
    "    non_instr_tuned_decoder_only_lm_dataset.train_test_split(\n",
    "        test_size=0.1, shuffle=True\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"non_instr_tuned_decoder_only_lm_dataset_train_val: \"\n",
    "    f\"{non_instr_tuned_decoder_only_lm_dataset_train_val }\"\n",
    ")\n",
    "\n",
    "# non-instruction tuned and seq2seq LMs\n",
    "non_instr_tuned_seq2seq_lm_dataset_train_val = (\n",
    "    non_instr_tuned_seq2seq_lm_dataset.train_test_split(test_size=0.1, shuffle=True)\n",
    ")\n",
    "print(\n",
    "    \"non_instr_tuned_seq2seq_lm_dataset_train_val: \"\n",
    "    f\"{non_instr_tuned_seq2seq_lm_dataset_train_val }\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set train/val specific transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.data.video import VideoPathHandler\n",
    "from pytorchvideo.transforms import UniformTemporalSubsample\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomRotation\n",
    "\n",
    "from train import extract_frames\n",
    "\n",
    "clip_path = \"../../ego4d/v2/clips/\"\n",
    "video_path_handler = VideoPathHandler()\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        UniformTemporalSubsample(8),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomRotation((-45, 45)),\n",
    "    ]\n",
    ")\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        UniformTemporalSubsample(8),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# instruction tuned and decoder only LMs\n",
    "instr_tuned_decoder_only_lm_dataset_train_val[\"train\"].set_transform(\n",
    "    partial(\n",
    "        extract_frames,\n",
    "        video_path_handler,\n",
    "        decoder_only_processor.image_processor,\n",
    "        clip_path,\n",
    "        video_transform=train_transform,\n",
    "    )\n",
    ")\n",
    "instr_tuned_decoder_only_lm_dataset_train_val[\"test\"].set_transform(\n",
    "    partial(\n",
    "        extract_frames,\n",
    "        video_path_handler,\n",
    "        decoder_only_processor.image_processor,\n",
    "        clip_path,\n",
    "        video_transform=val_transform,\n",
    "    )\n",
    ")\n",
    "\n",
    "# instruction tuned and seq2seq LMs\n",
    "instr_tuned_seq2seq_lm_dataset_train_val[\"train\"].set_transform(\n",
    "    partial(\n",
    "        extract_frames,\n",
    "        video_path_handler,\n",
    "        seq2seq_processor.image_processor,\n",
    "        clip_path,\n",
    "        video_transform=train_transform,\n",
    "    )\n",
    ")\n",
    "instr_tuned_seq2seq_lm_dataset_train_val[\"test\"].set_transform(\n",
    "    partial(\n",
    "        extract_frames,\n",
    "        video_path_handler,\n",
    "        seq2seq_processor.image_processor,\n",
    "        clip_path,\n",
    "        video_transform=val_transform,\n",
    "    )\n",
    ")\n",
    "\n",
    "# non-instruction tuned and decoder only LMs\n",
    "non_instr_tuned_decoder_only_lm_dataset_train_val[\"train\"].set_transform(\n",
    "    partial(\n",
    "        extract_frames,\n",
    "        video_path_handler,\n",
    "        decoder_only_processor.image_processor,\n",
    "        clip_path,\n",
    "        video_transform=train_transform,\n",
    "    )\n",
    ")\n",
    "non_instr_tuned_decoder_only_lm_dataset_train_val[\"test\"].set_transform(\n",
    "    partial(\n",
    "        extract_frames,\n",
    "        video_path_handler,\n",
    "        decoder_only_processor.image_processor,\n",
    "        clip_path,\n",
    "        video_transform=val_transform,\n",
    "    )\n",
    ")\n",
    "\n",
    "# non-instruction tuned and seq2seq LMs\n",
    "non_instr_tuned_seq2seq_lm_dataset_train_val[\"train\"].set_transform(\n",
    "    partial(\n",
    "        extract_frames,\n",
    "        video_path_handler,\n",
    "        seq2seq_processor.image_processor,\n",
    "        clip_path,\n",
    "        video_transform=train_transform,\n",
    "    )\n",
    ")\n",
    "non_instr_tuned_seq2seq_lm_dataset_train_val[\"test\"].set_transform(\n",
    "    partial(\n",
    "        extract_frames,\n",
    "        video_path_handler,\n",
    "        seq2seq_processor.image_processor,\n",
    "        clip_path,\n",
    "        video_transform=val_transform,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw three data points from `non_instr_tuned_decoder_only_lm_dataset` and run them through `VideoBlip2ForConditionalGeneration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from video_blip2 import VideoBlip2ForConditionalGeneration\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = VideoBlip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "from train import PROMPT\n",
    "\n",
    "print(\"non_instr_tuned_decoder_only_lm_dataset train\")\n",
    "for i, item in enumerate(\n",
    "    draw_random_items(non_instr_tuned_decoder_only_lm_dataset_train_val[\"train\"], 3)\n",
    "):\n",
    "    print(f\"input_ids: {item['input_ids']}\")\n",
    "    display(\n",
    "        display_gif(\n",
    "            item[\"pixel_values\"],\n",
    "            f\"non_instr_tuned_decoder_only_lm_dataset_train_{i}.gif\",\n",
    "        )\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            pixel_values=item[\"pixel_values\"].unsqueeze(0).to(device),\n",
    "            input_ids=decoder_only_processor.tokenizer(\n",
    "                PROMPT, return_tensors=\"pt\"\n",
    "            ).input_ids.to(device),\n",
    "        )\n",
    "    generated_text = decoder_only_processor.batch_decode(\n",
    "        generated_ids, skip_special_tokens=True\n",
    "    )[0].strip()\n",
    "    print(f\"generated_text: {generated_text}\")\n",
    "print(\"non_instr_tuned_decoder_only_lm_dataset val\")\n",
    "for i, item in enumerate(\n",
    "    draw_random_items(non_instr_tuned_decoder_only_lm_dataset_train_val[\"test\"], 3)\n",
    "):\n",
    "    print(f\"input_ids: {item['input_ids']}\")\n",
    "    display(\n",
    "        display_gif(\n",
    "            item[\"pixel_values\"], f\"non_instr_tuned_decoder_only_lm_dataset_val_{i}.gif\"\n",
    "        )\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            pixel_values=item[\"pixel_values\"].unsqueeze(0).to(device),\n",
    "            input_ids=decoder_only_processor.tokenizer(\n",
    "                PROMPT, return_tensors=\"pt\"\n",
    "            ).input_ids.to(device),\n",
    "        )\n",
    "    generated_text = decoder_only_processor.batch_decode(\n",
    "        generated_ids, skip_special_tokens=True\n",
    "    )[0].strip()\n",
    "    print(f\"generated_text: {generated_text}\")\n",
    "print(\"===================================\")\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw three data points from `instr_tuned_seq2seq_lm_dataset` and run them through `VideoBlip2ForConditionalGeneration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VideoBlip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-flan-t5-xl\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import INSTR_PROMPT\n",
    "\n",
    "print(\"instr_tuned_seq2seq_lm_dataset train\")\n",
    "for i, item in enumerate(\n",
    "    draw_random_items(instr_tuned_seq2seq_lm_dataset_train_val[\"train\"], 3)\n",
    "):\n",
    "    print(f\"input_ids: {item['input_ids']}\")\n",
    "    print(f\"labels: {item['labels']}\")\n",
    "    display(\n",
    "        display_gif(\n",
    "            item[\"pixel_values\"], f\"instr_tuned_seq2seq_lm_dataset_train_{i}.gif\"\n",
    "        )\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            pixel_values=item[\"pixel_values\"].unsqueeze(0).to(device),\n",
    "            input_ids=seq2seq_processor.tokenizer(\n",
    "                INSTR_PROMPT, return_tensors=\"pt\"\n",
    "            ).input_ids.to(device),\n",
    "        )\n",
    "    generated_text = seq2seq_processor.batch_decode(\n",
    "        generated_ids, skip_special_tokens=True\n",
    "    )[0].strip()\n",
    "    print(f\"generated_text: {generated_text}\")\n",
    "print(\"instr_tuned_seq2seq_lm_dataset val\")\n",
    "for i, item in enumerate(\n",
    "    draw_random_items(instr_tuned_seq2seq_lm_dataset_train_val[\"test\"], 3)\n",
    "):\n",
    "    print(f\"input_ids: {item['input_ids']}\")\n",
    "    print(f\"labels: {item['labels']}\")\n",
    "    display(\n",
    "        display_gif(item[\"pixel_values\"], f\"instr_tuned_seq2seq_lm_dataset_val_{i}.gif\")\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            pixel_values=item[\"pixel_values\"].unsqueeze(0).to(device),\n",
    "            input_ids=seq2seq_processor.tokenizer(\n",
    "                INSTR_PROMPT, return_tensors=\"pt\"\n",
    "            ).input_ids.to(device),\n",
    "        )\n",
    "    generated_text = seq2seq_processor.batch_decode(\n",
    "        generated_ids, skip_special_tokens=True\n",
    "    )[0].strip()\n",
    "    print(f\"generated_text: {generated_text}\")\n",
    "print(\"===================================\")\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fbev-jEv4LXUZ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
