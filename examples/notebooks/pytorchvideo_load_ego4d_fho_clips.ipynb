{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Ego4D FHO Clips Using PyTorchVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load `fho_main.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../../ego4d/v2/annotations/fho_main.json\") as f:\n",
    "    fho_main = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick an arbitrary action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = fho_main[\"videos\"][20]\n",
    "interval = video[\"annotated_intervals\"][2]\n",
    "action = interval[\"narrated_actions\"][4]\n",
    "\n",
    "print(f'video_uid: {video[\"video_uid\"]}')\n",
    "print(f'start_sec: {action[\"start_sec\"]}')\n",
    "print(f'end_sec: {action[\"end_sec\"]}')\n",
    "print(f'clip_uid: {interval[\"clip_uid\"]}')\n",
    "print(f'clip_start_sec: {action[\"clip_start_sec\"]}')\n",
    "print(f'clip_end_sec: {action[\"clip_end_sec\"]}')\n",
    "print(f'narration_text: {action[\"narration_text\"]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the action from the full video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def display_video(kind, uid, start_sec, end_sec):\n",
    "    html = f\"\"\"\n",
    "        <video id=\"video-{kind}-{uid}\" width=\"480\" height=\"320\" controls>\n",
    "            <source src=\"../../ego4d/v2/{kind}/{uid}.mp4\" type=\"video/mp4\">\n",
    "            Your browser does not support the video tag.\n",
    "        </video>\n",
    "        <script>\n",
    "            var video = document.getElementById('video-{kind}-{uid}');\n",
    "            video.currentTime = {start_sec};\n",
    "            video.addEventListener('timeupdate', function() {{\n",
    "                if (video.currentTime >= {end_sec}) {{\n",
    "                    video.pause();\n",
    "                }}\n",
    "            }});\n",
    "        </script>\n",
    "        \"\"\"\n",
    "\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "display_video(\"full_scale\", video[\"video_uid\"], action[\"start_sec\"], action[\"end_sec\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the action from the clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video(\n",
    "    \"clips\", interval[\"clip_uid\"], action[\"clip_start_sec\"], action[\"clip_end_sec\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They both mark the same action, so we can just use the clips, which are smaller and more wieldy.\n",
    "\n",
    "Now let's load the clip, and extract the frames corresponding to the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.data.video import VideoPathHandler\n",
    "\n",
    "video_path_handler = VideoPathHandler()\n",
    "\n",
    "# First, load the video corresponding to the clip\n",
    "video = video_path_handler.video_from_path(\n",
    "    f\"../../ego4d/v2/clips/{interval['clip_uid']}.mp4\"\n",
    ")\n",
    "\n",
    "# Now extract the clip corresponding to the action\n",
    "clip = video.get_clip(action[\"clip_start_sec\"], action[\"clip_end_sec\"])\n",
    "\n",
    "# frame tensor for the action\n",
    "# the action is 8 seconds, and the clip is 30fps, so 240 frames are extracted.\n",
    "# (C, T, H, W)\n",
    "print(clip[\"video\"].size())\n",
    "\n",
    "# audio\n",
    "print(clip[\"audio\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the video using `BlipImageProcessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.transforms import UniformTemporalSubsample\n",
    "from torchvision.transforms import Compose\n",
    "from transformers import Blip2Processor\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "transforms = Compose([UniformTemporalSubsample(8)])\n",
    "\n",
    "frames = transforms(clip[\"video\"])\n",
    "\n",
    "# print image processor options\n",
    "print(processor.image_processor)\n",
    "\n",
    "# treat the time dimension as the batch dimension\n",
    "processed_frames = processor.image_processor(\n",
    "    frames.permute(1, 0, 2, 3), return_tensors=\"pt\"\n",
    ")[\"pixel_values\"]\n",
    "\n",
    "# (T, C, H, W)\n",
    "print(f\"processed_frames.size(): {processed_frames.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look as a gif. Note that the colors will look all wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopted from https://huggingface.co/docs/transformers/v4.28.1/en/tasks/video_classification#visualize-the-preprocessed-video-for-better-debugging # noqa\n",
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "    \"\"\"Prepares a GIF from a video tensor.\n",
    "\n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_frames, num_channels, height, width).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for video_frame in video_tensor:\n",
    "        frames.append((video_frame.permute(1, 2, 0).numpy()).astype(np.uint8))\n",
    "    imageio.mimsave(filename, frames, \"GIF\")\n",
    "    return filename\n",
    "\n",
    "\n",
    "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "    \"\"\"Prepares and displays a GIF from a video tensor.\n",
    "\n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_frames, num_channels, height, width).\n",
    "    \"\"\"\n",
    "    gif_filename = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_filename)\n",
    "\n",
    "\n",
    "display_gif(processed_frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
