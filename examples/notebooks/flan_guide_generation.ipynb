{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide Generation Using FLAN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a kitchen video from ego4d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.data.video import VideoPathHandler\n",
    "\n",
    "video_path_handler = VideoPathHandler()\n",
    "video = video_path_handler.video_from_path(\n",
    "    \"../../ego4d/v2/full_scale/7f9f75fd-a660-4635-8890-239c6ad82023.mp4\"\n",
    ")\n",
    "clip = video.get_clip(27, 35)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `Salesforce/blip2-flan-t5-xxl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Blip2Processor\n",
    "\n",
    "from video_blip2.model import VideoBlip2ForConditionalGeneration\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\n",
    "model = VideoBlip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-flan-t5-xxl\", torch_dtype=torch.float16\n",
    ").to(device)\n",
    "print(model.generation_config)\n",
    "\n",
    "pixel_values = (\n",
    "    processor.image_processor(\n",
    "        clip[\"video\"][:, 0 : clip[\"video\"].size(1) : 30, ...].permute(1, 0, 2, 3),\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    .to(device, torch.float16)\n",
    "    .pixel_values.permute(1, 0, 2, 3)\n",
    "    .unsqueeze(0)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a conversational style reasoning task.\n",
    "\n",
    "Note that `flan-t5-xxl` with its 11B parameters is not \"big\" enough to perform zero-shot inference on unseen recipes, so the performance is not great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8: noqa\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "prompts = [\n",
    "    (\"What is the camera wearer currently doing?\", pixel_values),\n",
    "    (\n",
    "        \"\"\"Answer questions based on context below:\n",
    "\n",
    "The camera wearer is trying to make a mug cake following the recipe below:\n",
    "\n",
    "Recipe C: Mug Cake\n",
    "Ingredients\n",
    "2 Tablespoons all-purpose flour\n",
    "1.5 Tablespoons granulated sugar\n",
    "1/4 teaspoon baking powder\n",
    "Pinch salt\n",
    "2 teaspoons canola or vegetable oil\n",
    "2 Tablespoons water\n",
    "1/4 teaspoon vanilla extract\n",
    "Container of chocolate frosting (premade)\n",
    "\n",
    "Tools and Utensils\n",
    "measuring spoons small mixing bowl whisk\n",
    "paper cupcake liner 12-ounce coffee mug plate\n",
    "microwave\n",
    "zip-top bag, snack or sandwich size scissors\n",
    "spoon\n",
    "toothpick\n",
    "\n",
    "Steps\n",
    "1. Place the paper cupcake liner inside the mug. Set aside.\n",
    "2. Measure and add the flour, sugar, baking powder, and salt to the mixing bowl.\n",
    "3. Whisk to combine.\n",
    "4. Measure and add the oil, water, and vanilla to the bowl.\n",
    "5. Whisk batter until no lumps remain.\n",
    "6. Pour batter into prepared mug.\n",
    "7. Microwave the mug and batter on high power for 60 seconds.\n",
    "8. Check if the cake is done by inserting and toothpick into the center of the cake and then\n",
    "removing. If wet batter clings to the toothpick, microwave for an additional 5 seconds. If the\n",
    "toothpick comes out clean, continue.\n",
    "9. Invert the mug to release the cake onto a plate. Allow to cool until it is no longer hot to the\n",
    "touch, then carefully remove paper liner.\n",
    "10. While the cake is cooling, prepare to pipe the frosting. Scoop 4 spoonfuls of chocolate frosting\n",
    "into a zip-top bag and seal, removing as much air as possible.\n",
    "11. Use scissors to cut one corner from the bag to create a small opening 1‚ÅÑ4-inch in diameter.\n",
    "12. Squeeze the frosting through the opening to apply small dollops of frosting to the plate in a\n",
    "circle around the base of the cake.\n",
    "\n",
    "Here's the description of the current situation:\n",
    "Recipe steps the camera wearer has completed: none.\n",
    "The camera wearer is supposed to work on step 1 - Place the paper cupcake liner inside the mug. Set aside.\n",
    "\n",
    "Which step in the recipe is the camera wearer actually performing?\"\"\",\n",
    "        None,\n",
    "    ),\n",
    "    (\"Has the camera wearer followed the recipe correctly so far?\", None),\n",
    "    (\n",
    "        \"What instruction would you give to the camera wearer to correct their mistakes? If no instruction is necessary, answer with <None>.\",\n",
    "        None,\n",
    "    ),\n",
    "]\n",
    "generation_config = GenerationConfig(max_new_tokens=128, num_beams=4)\n",
    "\n",
    "context = \"\"\n",
    "for text, video in prompts:\n",
    "    print(text + \"\\n\")\n",
    "    context += text + \"\\n\"\n",
    "    inputs = processor(text=text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        if video is None:\n",
    "            generated_ids = model.language_model.generate(\n",
    "                **inputs, generation_config=generation_config\n",
    "            )\n",
    "        else:\n",
    "            inputs[\"pixel_values\"] = video\n",
    "            generated_ids = model.generate(\n",
    "                **inputs, generation_config=generation_config\n",
    "            )\n",
    "    answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    print(answer + \"\\n\")\n",
    "    context += answer + \"\\n\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-blip2-jEv4LXUZ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
