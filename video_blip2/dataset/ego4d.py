import json
import os
import random
import re
from collections.abc import Callable
from fractions import Fraction
from typing import Any

from pytorchvideo.data import ClipSampler, LabeledVideoDataset
from pytorchvideo.data.clip_sampling import ClipInfo


class RandomNarratedActionClipSampler(ClipSampler):
    def __init__(self) -> None:
        """The vast majority of narrated actions are 8 seconds long, and none
        are longer.

        So let's just sample 8-second clips.
        """
        super().__init__(8)
        self.shuffled_clip_indices: list[int] | None = None

    def __call__(
        self,
        last_clip_time: float | Fraction,
        video_duration: float | Fraction,
        annotation: dict[str, Any],
    ) -> ClipInfo:
        """Draw a random clip for a narrated action.

        :param last_clip_time: unused
        :param video_duration: unused
        :param annotation: narrated action data.
            See https://ego4d-data.org/docs/data/annotations-schemas/ for more details.
        """
        if self.shuffled_clip_indices is None:
            # first time sampling from this video, so create a shuffled list
            self.shuffled_clip_indices = list(
                range(len(annotation["narrated_actions"]))
            )
            random.shuffle(self.shuffled_clip_indices)

        clip_index = self.shuffled_clip_indices[self._current_clip_index]
        narrated_action = annotation["narrated_actions"][clip_index]
        self._current_clip_index += 1

        is_last_clip = False
        if self._current_clip_index == len(self.shuffled_clip_indices):
            is_last_clip = True

        # sample a clip 8 seconds around narration_time_sec
        # if narration_time_sec is less than 4 seconds, we start from 0
        clip_start_sec = max(
            Fraction(narrated_action["narration_timestamp_sec"])
            - self._clip_duration / 2,
            0,
        )

        # add 8 seconds to clip_start_sec
        # if clip_end_sec goes over the video duration, adjust clip_start_sec
        clip_end_sec = clip_start_sec + self._clip_duration
        video_duration_sec = Fraction(annotation["video_metadata"]["duration_sec"])
        if clip_end_sec > video_duration_sec:
            clip_end_sec = video_duration_sec
            clip_start_sec = clip_end_sec - self._clip_duration

        if is_last_clip:
            self.reset()

        return ClipInfo(
            clip_start_sec,
            clip_end_sec,
            clip_index,
            0,
            is_last_clip,
        )

    def reset(self) -> None:
        self._current_clip_index = 0
        self.shuffled_clip_indices = None


class Ego4dFHOMainDataset(LabeledVideoDataset):
    C_REGEX = re.compile(r"^\#C C", re.IGNORECASE)

    def __init__(
        self,
        annotation_path: str,
        split_path: str,
        video_dir_path: str,
        transform: Callable[[dict], Any] | None = None,
    ) -> None:
        """
        :param annotation_path: path to the main annotation file, e.g., `fho_main.json`.
        :param split_path: path to video split file generated by
            `scripts/split_train_val.py`.
        :param video_path: path to video dir
        :param transform: optional transform function
        """
        with open(annotation_path) as f:
            annotations = json.load(f)

        # create a dict video_uid => video
        video_dict = {video["video_uid"]: video for video in annotations["videos"]}

        with open(split_path) as f:
            split_data = json.load(f)

        self.split = split_data["split"]

        def _transform(item: dict) -> Any:
            """The first transform function that formats `narrated_actions`."""
            narrated_actions = item.pop("narrated_actions")
            item.update(narrated_actions[item["clip_index"]])
            if transform is not None:
                item = transform(item)
            return item

        super().__init__(
            [
                (
                    os.path.join(video_dir_path, video_uid + ".mp4"),
                    {
                        "narrated_actions": [
                            {
                                "narration_timestamp_sec": action[
                                    "narration_timestamp_sec"
                                ],
                                "narration_text": action["narration_text"],
                            }
                            for interval in video_dict[video_uid]["annotated_intervals"]
                            for action in interval["narrated_actions"]
                            if not action["is_rejected"]
                            and action["is_valid_action"]
                            and self.C_REGEX.match(action["narration_text"])
                        ],
                        "video_uid": video_uid,
                        "video_metadata": video_dict[video_uid]["video_metadata"],
                    },
                )
                for video_uid in split_data["videos"]
            ],
            RandomNarratedActionClipSampler(),
            transform=_transform,
            decode_audio=False,
        )
